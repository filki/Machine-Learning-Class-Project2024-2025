{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# @title Komórka 1: Instalacja i import bibliotek, pobieranie zasobów NLTK\n",
    "# Zainstaluj/zaktualizuj potrzebne biblioteki\n",
    "!pip install -q gensim pandas nltk scikit-learn matplotlib # Dodajemy gensim\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import gc # Garbage Collector\n",
    "import random\n",
    "\n",
    "# NLTK do przetwarzania tekstu\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Gensim do modelowania tematów i spójności\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import LsiModel, LdaMulticore, CoherenceModel\n",
    "\n",
    "# Do obsługi ostrzeżeń\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning) # Ignoruj ostrzeżenia o przestarzałych funkcjach (częste w gensim)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# --- Konfiguracja ---\n",
    "DATA_FILE_PATH = '../../data/dataset_combined.csv' # Ścieżka do pliku danych\n",
    "OUTPUT_DIR = '../../results/topic_modeling_results_gensim' # Nowy katalog na wyniki z gensim\n",
    "NUM_TOPICS_LIST = [75, 100] # Liczby tematów do analizy\n",
    "TOP_N_WORDS = 10 # Ile słów zapisać dla każdego tematu\n",
    "\n",
    "# --- Pobieranie zasobów NLTK ---\n",
    "print(\"Pobieranie zasobów NLTK (punkt, stopwords, wordnet)...\")\n",
    "try:\n",
    "    import ssl\n",
    "    try:\n",
    "        _create_unverified_https_context = ssl._create_unverified_context\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    else:\n",
    "        ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"Zasoby NLTK pobrane.\")\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił błąd podczas pobierania zasobów NLTK: {e}\")\n",
    "\n",
    "# Utworzenie katalogu na wyniki, jeśli nie istnieje\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-GUGtHWuE5-",
    "outputId": "828445a7-58ff-40aa-897b-cfed9273d66d",
    "ExecuteTime": {
     "end_time": "2025-05-06T09:29:42.086242Z",
     "start_time": "2025-05-06T09:29:29.918496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Pobieranie zasobów NLTK (punkt, stopwords, wordnet)...\n",
      "Zasoby NLTK pobrane.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Komórka 2: Wczytanie i wstępne przygotowanie danych (bez zmian)\n",
    "print(f\"Wczytywanie danych z pliku: {DATA_FILE_PATH}...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE_PATH, low_memory=False)\n",
    "    print(f\"Wczytano {len(df)} wierszy.\")\n",
    "    print(f\"Czas wczytywania: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    print(\"\\nDostępne kolumny:\", df.columns.tolist())\n",
    "    text_column = 'review'\n",
    "    if text_column not in df.columns:\n",
    "        raise ValueError(f\"Nie znaleziono kolumny '{text_column}' w pliku CSV.\")\n",
    "\n",
    "    print(f\"\\nLiczba brakujących wartości w kolumnie '{text_column}': {df[text_column].isnull().sum()}\")\n",
    "    df.dropna(subset=[text_column], inplace=True)\n",
    "    df[text_column] = df[text_column].astype(str)\n",
    "    print(f\"Liczba wierszy po usunięciu brakujących wartości: {len(df)}\")\n",
    "\n",
    "    # --- Opcjonalne próbkowanie dla szybszych testów ---\n",
    "    # sample_size = 10000 # Mała próbka do testów\n",
    "    # if len(df) > sample_size:\n",
    "    #     print(f\"\\nZmniejszanie zbioru danych do {sample_size} próbek dla testów...\")\n",
    "    #     df = df.sample(n=sample_size, random_state=42)\n",
    "    #     gc.collect()\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    texts = df[text_column].tolist()\n",
    "    print(f\"\\nPrzykładowa recenzja (przed przetwarzaniem):\\n{texts[0][:500]}...\")\n",
    "\n",
    "    del df # Usuń DataFrame, aby zwolnić pamięć\n",
    "    gc.collect()\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"BŁĄD: Nie znaleziono pliku {DATA_FILE_PATH}.\")\n",
    "    texts = None\n",
    "except ValueError as ve:\n",
    "    print(f\"BŁĄD: {ve}\")\n",
    "    texts = None\n",
    "except Exception as e:\n",
    "    print(f\"Wystąpił nieoczekiwany błąd podczas wczytywania danych: {e}\")\n",
    "    texts = None"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5mYvQk7uF5b",
    "outputId": "52c284e0-ed17-4074-9bf4-79a86ffb934f",
    "ExecuteTime": {
     "end_time": "2025-05-06T09:29:46.420544Z",
     "start_time": "2025-05-06T09:29:42.148377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytywanie danych z pliku: ../../data/dataset_combined.csv...\n",
      "Wczytano 253073 wierszy.\n",
      "Czas wczytywania: 4.05s\n",
      "\n",
      "Dostępne kolumny: ['recommendationid', 'author', 'language', 'review', 'timestamp_created', 'timestamp_updated', 'voted_up', 'votes_up', 'votes_funny', 'weighted_vote_score', 'comment_count', 'steam_purchase', 'received_for_free', 'written_during_early_access', 'primarily_steam_deck', 'appid', 'fetch_date', 'review_type', 'total_reviews', 'timestamp_dev_responded', 'developer_response']\n",
      "\n",
      "Liczba brakujących wartości w kolumnie 'review': 407\n",
      "Liczba wierszy po usunięciu brakujących wartości: 252666\n",
      "\n",
      "Przykładowa recenzja (przed przetwarzaniem):\n",
      "As we get older, playing games feels boring, even though there is something exciting in the game, at least 1-3 hours then we will feel bored again, but it's different with this game, once you play 1 hour you will definitely be attached and really want to spend the whole day playing this game. \n",
      "\n",
      "This game succeeded in making me feel like a child again who used to spend all my time playing iconic games\n",
      "\n",
      "and I want to say a special thank you to whoever made panam\n",
      "\n",
      "(I'm not that rich to buy this gam...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Komórka 3: Preprocessing tekstu i tworzenie słownika/korpusu Gensim\n",
    "\n",
    "# Inicjalizacja lematyzatora i listy stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "custom_stop_words = {\n",
    "    'game', 'games', 'play', 'played', 'playing', 'player', 'players',\n",
    "    'steam', 'review', 'reviews', 'recommend', 'recommended', 'recommendation',\n",
    "    'hour', 'hours', 'hr', 'hrs',\n",
    "    'like', 'good', 'bad', 'great', 'best', 'worst', 'fun', 'nice', 'well',\n",
    "    'really', 'much', 'many', 'lot', 'also', 'get', 'got', 'make', 'made',\n",
    "    'time', 'people', 'would', 'one', 'even', 'go', 'going', 'want', 'say',\n",
    "    'thing', 'think', 'know', 'see', 'come', 'back', 'use', 'still',\n",
    "    'early', 'access', 'ea', 'dlc', 'buy', 'bought', 'purchase', 'price', 'worth',\n",
    "    'pc', 'console', 'etc', 've', 're' # Dodano kilka typowych skrótów/wypełniaczy\n",
    "}\n",
    "stop_words = set(stopwords.words('english')).union(custom_stop_words)\n",
    "print(f\"Rozmiar słownika stopwords: {len(stop_words)}\")\n",
    "\n",
    "# ZMODYFIKOWANA funkcja preprocessingu ZWRACAJĄCA LISTĘ TOKENÓW\n",
    "def preprocess_text_gensim(text):\n",
    "    \"\"\"\n",
    "    Czyści tekst i zwraca listę tokenów (słów).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text) # Usuń wszystko poza literami i spacjami\n",
    "    tokens = word_tokenize(text)\n",
    "    processed_tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word not in stop_words and len(word) > 2\n",
    "    ]\n",
    "    return processed_tokens # Zwraca listę tokenów\n",
    "\n",
    "# Zastosowanie preprocessingu\n",
    "if texts:\n",
    "    print(\"\\nRozpoczynanie preprocessingu tekstów dla Gensim...\")\n",
    "    start_time = time.time()\n",
    "    processed_docs = [preprocess_text_gensim(doc) for doc in texts]\n",
    "    print(f\"Preprocessing zakończony. Czas: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "    # Usuń puste listy tokenów, które mogły powstać\n",
    "    processed_docs = [doc for doc in processed_docs if doc]\n",
    "    print(f\"Liczba dokumentów po preprocessingu i usunięciu pustych: {len(processed_docs)}\")\n",
    "\n",
    "    # Usuń oryginalne teksty, aby zwolnić pamięć\n",
    "    del texts\n",
    "    gc.collect()\n",
    "\n",
    "    if processed_docs:\n",
    "        print(f\"\\nPrzykładowy dokument (po preprocessingu, jako lista tokenów):\\n{processed_docs[0][:20]}...\")\n",
    "\n",
    "        # --- Tworzenie słownika i korpusu Gensim ---\n",
    "        print(\"\\nTworzenie słownika Gensim...\")\n",
    "        dictionary = corpora.Dictionary(processed_docs)\n",
    "        print(f\"Rozmiar słownika przed filtrowaniem: {len(dictionary)}\")\n",
    "\n",
    "        # Filtrowanie ekstremalnych słów (podobnie do min_df, max_df, max_features)\n",
    "        dictionary.filter_extremes(\n",
    "            no_below=5,      # Ignoruj słowa występujące w mniej niż 5 dokumentach\n",
    "            no_above=0.7,    # Ignoruj słowa występujące w więcej niż 70% dokumentów\n",
    "            keep_n=10000     # Zachowaj max 10000 najczęstszych słów po filtrowaniu\n",
    "        )\n",
    "        dictionary.compactify() # Przypisz nowe, ciągłe ID po usunięciu słów\n",
    "        print(f\"Rozmiar słownika po filtrowaniu: {len(dictionary)}\")\n",
    "\n",
    "        print(\"\\nTworzenie korpusu Bag-of-Words (BoW)...\")\n",
    "        # Tworzenie korpusu (lista list krotek: (id_slowa, liczba_wystapien))\n",
    "        corpus_bow = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "        # Sprawdzenie, czy korpus nie jest pusty\n",
    "        if not corpus_bow or not any(corpus_bow):\n",
    "             print(\"BŁĄD: Korpus BoW jest pusty. Sprawdź wyniki preprocessingu.\")\n",
    "             # Ustaw zmienne na None, aby kolejne komórki nie próbowały ich użyć\n",
    "             processed_docs = None\n",
    "             dictionary = None\n",
    "             corpus_bow = None\n",
    "        else:\n",
    "             print(f\"Utworzono korpus BoW dla {len(corpus_bow)} dokumentów.\")\n",
    "             print(f\"Przykładowy wpis w korpusie BoW (dla pierwszego dokumentu):\\n{corpus_bow[0][:10]}...\") # Pokaż początek\n",
    "\n",
    "             # Utworzenie korpusu TF-IDF (potrzebny dla LSA)\n",
    "             print(\"\\nTworzenie modelu TF-IDF i korpusu TF-IDF...\")\n",
    "             tfidf_model = gensim.models.TfidfModel(corpus_bow, id2word=dictionary)\n",
    "             corpus_tfidf = tfidf_model[corpus_bow]\n",
    "             print(f\"Utworzono korpus TF-IDF.\")\n",
    "             print(f\"Przykładowy wpis w korpusie TF-IDF (dla pierwszego dokumentu):\\n{corpus_tfidf[0][:10]}...\")\n",
    "\n",
    "    else:\n",
    "        print(\"Lista 'processed_docs' jest pusta po preprocessingu.\")\n",
    "        dictionary = None\n",
    "        corpus_bow = None\n",
    "        corpus_tfidf = None\n",
    "\n",
    "else:\n",
    "    print(\"Brak tekstów wejściowych do przetworzenia.\")\n",
    "    processed_docs = None\n",
    "    dictionary = None\n",
    "    corpus_bow = None\n",
    "    corpus_tfidf = None\n",
    "\n",
    "gc.collect() # Zwolnij pamięć"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5rPgX-nlvrym",
    "outputId": "09af198f-b0ea-46a1-9f03-61d7ea1ed162",
    "ExecuteTime": {
     "end_time": "2025-05-06T09:31:39.948271Z",
     "start_time": "2025-05-06T09:29:46.583081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rozmiar słownika stopwords: 262\n",
      "\n",
      "Rozpoczynanie preprocessingu tekstów dla Gensim...\n",
      "Preprocessing zakończony. Czas: 97.87s\n",
      "Liczba dokumentów po preprocessingu i usunięciu pustych: 243845\n",
      "\n",
      "Przykładowy dokument (po preprocessingu, jako lista tokenów):\n",
      "['older', 'feel', 'boring', 'though', 'something', 'exciting', 'least', 'feel', 'bored', 'different', 'definitely', 'attached', 'spend', 'whole', 'day', 'succeeded', 'making', 'feel', 'child', 'used']...\n",
      "\n",
      "Tworzenie słownika Gensim...\n",
      "Rozmiar słownika przed filtrowaniem: 105275\n",
      "Rozmiar słownika po filtrowaniu: 10000\n",
      "\n",
      "Tworzenie korpusu Bag-of-Words (BoW)...\n",
      "Utworzono korpus BoW dla 243845 dokumentów.\n",
      "Przykładowy wpis w korpusie BoW (dla pierwszego dokumentu):\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1)]...\n",
      "\n",
      "Tworzenie modelu TF-IDF i korpusu TF-IDF...\n",
      "Utworzono korpus TF-IDF.\n",
      "Przykładowy wpis w korpusie TF-IDF (dla pierwszego dokumentu):\n",
      "[(0, 0.23147332771606596), (1, 0.18586613368597454), (2, 0.11746156489868109), (3, 0.19396926193783895), (4, 0.25515704282501384), (5, 0.14561055424332425), (6, 0.27231551116825287), (7, 0.11236566085999895), (8, 0.15454675528729983), (9, 0.19654338357653875)]...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Komórka 4: Funkcja do modelowania tematów Gensim (LSA/LDA) i obliczania spójności C_v\n",
    "\n",
    "def run_gensim_topic_modeling(processed_docs, dictionary, corpus, method, n_topics, top_n_words, output_dir):\n",
    "    \"\"\"\n",
    "    Wykonuje modelowanie tematów (LSA lub LDA) używając Gensim,\n",
    "    oblicza spójność C_v i zapisuje wyniki do pliku CSV.\n",
    "\n",
    "    Args:\n",
    "        processed_docs (list of list of str): Lista dokumentów jako listy tokenów (potrzebna do CoherenceModel).\n",
    "        dictionary (gensim.corpora.Dictionary): Słownik Gensim.\n",
    "        corpus (iterable): Korpus Gensim (BoW dla LDA, TF-IDF dla LSA).\n",
    "        method (str): 'lsa' lub 'lda'.\n",
    "        n_topics (int): Liczba tematów do wyodrębnienia.\n",
    "        top_n_words (int): Liczba najważniejszych słów do zapisania dla każdego tematu.\n",
    "        output_dir (str): Katalog do zapisu pliku CSV.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not processed_docs or not dictionary or not corpus:\n",
    "        print(f\"BŁĄD: Brak danych wejściowych (processed_docs, dictionary lub corpus) dla metody {method.upper()}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Rozpoczynanie analizy {method.upper()} dla {n_topics} tematów ---\")\n",
    "    start_time = time.time()\n",
    "    model = None # Inicjalizacja zmiennej model\n",
    "\n",
    "    try:\n",
    "        # 1. Trenowanie modelu\n",
    "        print(\"Trenowanie modelu...\")\n",
    "        model_train_time = time.time()\n",
    "        if method == 'lsa':\n",
    "            # LSI (LSA) Model - używa korpusu TF-IDF\n",
    "            # Upewnij się, że n_topics nie jest zbyt duże (max to liczba cech w słowniku)\n",
    "            effective_n_topics = min(n_topics, len(dictionary))\n",
    "            if effective_n_topics < n_topics:\n",
    "                 print(f\"  Ostrzeżenie: Zmniejszono liczbę tematów z {n_topics} do {effective_n_topics} (maks. rozmiar słownika).\")\n",
    "            if effective_n_topics <= 0:\n",
    "                 print(f\"  BŁĄD: Nie można wytrenować modelu LSA z {effective_n_topics} tematami.\")\n",
    "                 return\n",
    "\n",
    "            model = LsiModel(\n",
    "                corpus=corpus, # Użyj corpus_tfidf przekazanego do funkcji\n",
    "                id2word=dictionary,\n",
    "                num_topics=effective_n_topics\n",
    "            )\n",
    "        elif method == 'lda':\n",
    "            # LDA Model - używa korpusu BoW\n",
    "            # LdaMulticore jest szybsze, jeśli dostępne są rdzenie\n",
    "            # Użyj workers=1 dla stabilności w Colab (unikanie problemów z pamięcią/dyskiem)\n",
    "            effective_n_topics = n_topics\n",
    "            if effective_n_topics <= 0:\n",
    "                 print(f\"  BŁĄD: Nie można wytrenować modelu LDA z {effective_n_topics} tematami.\")\n",
    "                 return\n",
    "\n",
    "            model = LdaMulticore(\n",
    "                corpus=corpus, # Użyj corpus_bow przekazanego do funkcji\n",
    "                id2word=dictionary,\n",
    "                num_topics=effective_n_topics,\n",
    "                random_state=42,\n",
    "                chunksize=100,      # Liczba dokumentów przetwarzanych w jednej iteracji\n",
    "                passes=10,          # Liczba przebiegów przez korpus\n",
    "                iterations=50,      # Maksymalna liczba iteracji w ramach przebiegu (dla zbieżności)\n",
    "                workers=6           # WAŻNE DLA COLAB: Użyj 1 rdzenia, aby uniknąć błędów pamięci/dysku\n",
    "                # per_word_topics=True # Potrzebne do niektórych zaawansowanych analiz, ale zużywa więcej pamięci\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Nieprawidłowa metoda. Wybierz 'lsa' lub 'lda'.\")\n",
    "        print(f\"Model wytrenowany. Czas trenowania: {time.time() - model_train_time:.2f}s\")\n",
    "\n",
    "        # 2. Obliczanie spójności C_v\n",
    "        print(\"Obliczanie spójności C_v...\")\n",
    "        coherence_time = time.time()\n",
    "        coherence_model = CoherenceModel(\n",
    "            model=model,\n",
    "            texts=processed_docs, # WAŻNE: Potrzebuje oryginalnych tokenizowanych tekstów\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        print(f\"Spójność C_v: {coherence_score:.4f}\")\n",
    "        print(f\"Czas obliczania spójności: {time.time() - coherence_time:.2f}s\")\n",
    "\n",
    "        # 3. Ekstrakcja i zapis wyników\n",
    "        print(\"Ekstrakcja tematów i zapisywanie wyników...\")\n",
    "        results_time = time.time()\n",
    "        # Pobierz tematy w wymaganym formacie\n",
    "        # formatted=False zwraca listę krotek (słowo, wynik)\n",
    "        topics = model.show_topics(num_topics=-1, num_words=top_n_words, formatted=False)\n",
    "\n",
    "        topic_results = []\n",
    "        for topic_id, word_score_pairs in topics:\n",
    "            words = [word for word, score in word_score_pairs]\n",
    "            scores = [score for word, score in word_score_pairs]\n",
    "            topic_results.append({\n",
    "                'topic_id': topic_id,\n",
    "                'words': str(words),   # Konwersja listy słów na string\n",
    "                'scores': str(scores)  # Konwersja listy wyników na string\n",
    "            })\n",
    "\n",
    "        # Utwórz DataFrame i zapisz do CSV\n",
    "        results_df = pd.DataFrame(topic_results)\n",
    "        output_filename = os.path.join(output_dir, f\"{method}_{n_topics}_topics_gensim.csv\") # Dodano _gensim do nazwy\n",
    "        results_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "\n",
    "        print(f\"Wyniki zapisane do pliku: {output_filename}\")\n",
    "        print(f\"Czas ekstrakcji i zapisu: {time.time() - results_time:.2f}s\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Wystąpił błąd podczas analizy {method.upper()} dla {n_topics} tematów: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Wydrukuj pełny traceback błędu\n",
    "\n",
    "    finally:\n",
    "        # Zawsze próbuj zwolnić pamięć\n",
    "        print(f\"--- Analiza {method.upper()} dla {n_topics} tematów zakończona. Całkowity czas: {time.time() - start_time:.2f}s ---\")\n",
    "        del model # Usuń model, aby zwolnić pamięć\n",
    "        if 'coherence_model' in locals():\n",
    "            del coherence_model\n",
    "        if 'results_df' in locals():\n",
    "            del results_df\n",
    "        if 'topic_results' in locals():\n",
    "            del topic_results\n",
    "        gc.collect()"
   ],
   "metadata": {
    "id": "Uu0qGTq3vwjX",
    "ExecuteTime": {
     "end_time": "2025-05-06T09:31:40.042866Z",
     "start_time": "2025-05-06T09:31:40.022005Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrGtVqr2vzpe",
    "outputId": "cbde3e93-8276-4da3-a81d-c3cd7e9760bc",
    "ExecuteTime": {
     "end_time": "2025-05-06T11:38:24.823794Z",
     "start_time": "2025-05-06T09:31:40.106755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @title Komórka 5: Uruchomienie analizy Gensim dla LSA i LDA\n",
    "\n",
    "# Sprawdź, czy potrzebne zmienne istnieją i nie są puste\n",
    "if ('processed_docs' in locals() and processed_docs and\n",
    "    'dictionary' in locals() and dictionary and\n",
    "    'corpus_bow' in locals() and corpus_bow and\n",
    "    'corpus_tfidf' in locals() and corpus_tfidf):\n",
    "\n",
    "    # --- Uruchomienie dla LSA (używa corpus_tfidf) ---\n",
    "    print(\"\\n=== Rozpoczynanie analizy LSA (Gensim) ===\")\n",
    "    for n_topics in NUM_TOPICS_LIST:\n",
    "        run_gensim_topic_modeling(processed_docs, dictionary, corpus_tfidf, 'lsa', n_topics, TOP_N_WORDS, OUTPUT_DIR)\n",
    "        gc.collect()\n",
    "\n",
    "    # --- Uruchomienie dla LDA (używa corpus_bow) ---\n",
    "    print(\"\\n=== Rozpoczynanie analizy LDA (Gensim) ===\")\n",
    "    for n_topics in NUM_TOPICS_LIST:\n",
    "        run_gensim_topic_modeling(processed_docs, dictionary, corpus_bow, 'lda', n_topics, TOP_N_WORDS, OUTPUT_DIR)\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\nWszystkie analizy Gensim zakończone.\")\n",
    "    print(f\"Wyniki znajdują się w katalogu: {OUTPUT_DIR}\")\n",
    "\n",
    "    # Wyświetlenie listy plików w katalogu wyników\n",
    "    print(\"\\nWygenerowane pliki:\")\n",
    "    try:\n",
    "        for filename in os.listdir(OUTPUT_DIR):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                print(f\"- {filename}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Katalog '{OUTPUT_DIR}' nie został znaleziony.\")\n",
    "\n",
    "else:\n",
    "    print(\"Nie można uruchomić modelowania tematów Gensim.\")\n",
    "    print(\"Sprawdź, czy poprzednie komórki wykonały się poprawnie i czy zmienne\")\n",
    "    print(\"'processed_docs', 'dictionary', 'corpus_bow', 'corpus_tfidf' zostały utworzone.\")\n",
    "\n",
    "# Opcjonalnie: Spakuj wyniki\n",
    "# import shutil\n",
    "# try:\n",
    "#     shutil.make_archive('topic_modeling_results_gensim', 'zip', OUTPUT_DIR)\n",
    "#     print(\"\\nWyniki Gensim spakowane do pliku topic_modeling_results_gensim.zip\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Błąd podczas pakowania wyników: {e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Rozpoczynanie analizy LSA (Gensim) ===\n",
      "\n",
      "--- Rozpoczynanie analizy LSA dla 75 tematów ---\n",
      "Trenowanie modelu...\n",
      "Model wytrenowany. Czas trenowania: 46.06s\n",
      "Obliczanie spójności C_v...\n",
      "Spójność C_v: 0.3024\n",
      "Czas obliczania spójności: 61.83s\n",
      "Ekstrakcja tematów i zapisywanie wyników...\n",
      "Wyniki zapisane do pliku: ../../results/topic_modeling_results_gensim/lsa_75_topics_gensim.csv\n",
      "Czas ekstrakcji i zapisu: 0.04s\n",
      "--- Analiza LSA dla 75 tematów zakończona. Całkowity czas: 107.94s ---\n",
      "\n",
      "--- Rozpoczynanie analizy LSA dla 100 tematów ---\n",
      "Trenowanie modelu...\n",
      "Model wytrenowany. Czas trenowania: 50.46s\n",
      "Obliczanie spójności C_v...\n",
      "Spójność C_v: 0.2978\n",
      "Czas obliczania spójności: 77.23s\n",
      "Ekstrakcja tematów i zapisywanie wyników...\n",
      "Wyniki zapisane do pliku: ../../results/topic_modeling_results_gensim/lsa_100_topics_gensim.csv\n",
      "Czas ekstrakcji i zapisu: 0.03s\n",
      "--- Analiza LSA dla 100 tematów zakończona. Całkowity czas: 127.73s ---\n",
      "\n",
      "=== Rozpoczynanie analizy LDA (Gensim) ===\n",
      "\n",
      "--- Rozpoczynanie analizy LDA dla 75 tematów ---\n",
      "Trenowanie modelu...\n",
      "Model wytrenowany. Czas trenowania: 3300.17s\n",
      "Obliczanie spójności C_v...\n",
      "Spójność C_v: 0.3434\n",
      "Czas obliczania spójności: 497.47s\n",
      "Ekstrakcja tematów i zapisywanie wyników...\n",
      "Wyniki zapisane do pliku: ../../results/topic_modeling_results_gensim/lda_75_topics_gensim.csv\n",
      "Czas ekstrakcji i zapisu: 0.02s\n",
      "--- Analiza LDA dla 75 tematów zakończona. Całkowity czas: 3797.67s ---\n",
      "\n",
      "--- Rozpoczynanie analizy LDA dla 100 tematów ---\n",
      "Trenowanie modelu...\n",
      "Model wytrenowany. Czas trenowania: 2969.97s\n",
      "Obliczanie spójności C_v...\n",
      "Spójność C_v: 0.3065\n",
      "Czas obliczania spójności: 596.61s\n",
      "Ekstrakcja tematów i zapisywanie wyników...\n",
      "Wyniki zapisane do pliku: ../../results/topic_modeling_results_gensim/lda_100_topics_gensim.csv\n",
      "Czas ekstrakcji i zapisu: 0.03s\n",
      "--- Analiza LDA dla 100 tematów zakończona. Całkowity czas: 3566.61s ---\n",
      "\n",
      "Wszystkie analizy Gensim zakończone.\n",
      "Wyniki znajdują się w katalogu: ../../results/topic_modeling_results_gensim\n",
      "\n",
      "Wygenerowane pliki:\n",
      "- lda_75_topics_gensim.csv\n",
      "- lsa_100_topics_gensim.csv\n",
      "- lda_100_topics_gensim.csv\n",
      "- lsa_75_topics_gensim.csv\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ]
}
